# ===============================================================
# í”„ë¡œê·¸ë¨ëª…: text_split.py
# ê°œìš”: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ë“¤ì–´ì„œ ì²­í‚¹í›„, jsoníŒŒì¼ë¡œ ì €ì¥
# ì´ë ¥: 2025.12.08 ìµœì´ˆ ì‘ì„±
# ê¸°íƒ€:
# ===============================================================
import os
import json
import re
import uuid

# -----------------------------------------------
# âš™ï¸ ì²­í‚¹ ì„¤ì • â€” ì›í•˜ë©´ ê°’ë§Œ ë°”ê¿”ì„œ ì¬ì¡°ì • ê°€ëŠ¥
# -----------------------------------------------
MAX_CHARS = 2000  # chunk ìµœëŒ€ ê¸¸ì´ (ë¬¸ì ê¸°ì¤€)
MIN_CHARS = 600  # ë„ˆë¬´ ì§§ì€ chunk ë°©ì§€
OVERLAP_CHARS = 250  # ê²¹ì¹¨ ë²”ìœ„
SECTION_BOOST = True  # [SECTION] ë“±ì¥ ì‹œ chunk ê²½ê³„ ê°•í™”

# -----------------------------------------------
# ğŸ“Œ ì…ë ¥ / ì¶œë ¥
# -----------------------------------------------
input_file = r"C:\Users\mungm\Documents\ai_engineer\genai_rag\data\2024ë…„ì›ì²œì§•ìˆ˜ì˜ë¬´ìë¥¼ ìœ„í•œ ì—°ë§ì •ì‚°ì‹ ê³ ì•ˆë‚´_pypdfloader_processing.txt"
base_name = os.path.splitext(os.path.basename(input_file))[0]
save_dir = os.path.dirname(input_file)
output_file = os.path.join(save_dir, f"{base_name}_chunks.json")

# -----------------------------------------------
# ğŸ”¹ TXT ì½ê¸°
# -----------------------------------------------
with open(input_file, "r", encoding="utf-8") as f:
    text = f.read()

# -----------------------------------------------
# ğŸ”¹ ë¬¸ë‹¨ ê¸°ì¤€ ë¶„ë¦¬
# -----------------------------------------------
paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

chunks = []
current = ""


def flush_chunk():
    """í˜„ì¬ chunkë¥¼ chunks ë¦¬ìŠ¤íŠ¸ì— ì €ì¥"""
    if len(current.strip()) == 0:
        return
    if len(current) < MIN_CHARS and chunks:
        # ì§ì „ chunkì— ë³‘í•©
        chunks[-1]["chunk"] += "\n" + current
    else:
        chunks.append({"chunk_id": str(uuid.uuid4()), "chunk": current.strip()})


# -----------------------------------------------
# ğŸ”¹ 1ì°¨ ì²­í‚¹: ë¬¸ë‹¨ â†’ chunk
# -----------------------------------------------
for para in paragraphs:
    # SECTIONì´ë©´ ìš°ì„ ì ìœ¼ë¡œ chunk ì¢…ë£Œ
    if SECTION_BOOST and para.startswith("[SECTION]") and len(current) > 0:
        flush_chunk()
        current = para
        continue

    # ê·¸ëƒ¥ ì´ì–´ ì“°ê¸°
    if len(current) + len(para) + 1 <= MAX_CHARS:
        current += ("\n" if current else "") + para
    else:
        flush_chunk()
        current = para

# ë§ˆì§€ë§‰ ì”ì—¬ chunk ì €ì¥
flush_chunk()

# -----------------------------------------------
# ğŸ”¹ 2ì°¨ ì²­í‚¹: chunkê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° â†’ í† ë§‰ ë¶„í•  + overlap
# -----------------------------------------------
final_chunks = []
for ch in chunks:
    content = ch["chunk"]
    if len(content) <= MAX_CHARS:
        final_chunks.append(ch)
        continue

    start = 0
    while start < len(content):
        end = start + MAX_CHARS
        piece = content[start:end]

        final_chunks.append({"chunk_id": str(uuid.uuid4()), "chunk": piece.strip()})

        start = end - OVERLAP_CHARS  # ì˜¤ë²„ë©
        if start < 0:
            start = 0

# -----------------------------------------------
# ğŸ”¹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (optional)
#  - [SECTION] ìë™ ì¶”ì¶œ
# -----------------------------------------------
for ch in final_chunks:
    m = re.search(r"\[SECTION\]\s*(.+)", ch["chunk"])
    ch["section"] = m.group(1).strip() if m else None
    ch["source_file"] = os.path.basename(input_file)
    ch["length"] = len(ch["chunk"])

# -----------------------------------------------
# ğŸ”¹ JSON ì €ì¥
# -----------------------------------------------
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(final_chunks, f, ensure_ascii=False, indent=2)

print("ì²­í‚¹ ì™„ë£Œ!")
print("ì´ chunk ìˆ˜:", len(final_chunks))
print("ì €ì¥ íŒŒì¼:", output_file)
