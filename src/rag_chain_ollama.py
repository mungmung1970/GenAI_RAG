# rag_chain_ollama_optionB.py
# (OpenAI Embedding + OpenAI Reranker + Ollama Final Answer + Context Length Control)

import os
import yaml
import json
import requests
import logging
from pathlib import Path
from dotenv import load_dotenv

from elasticsearch import Elasticsearch
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)
log = logging.getLogger("rag")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë° LangSmith ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

os.environ["LANGCHAIN_TRACING_V2"] = os.getenv("LANGSMITH_TRACING", "true")
os.environ["LANGCHAIN_ENDPOINT"] = os.getenv("LANGSMITH_ENDPOINT")
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGSMITH_API_KEY")
os.environ["LANGCHAIN_PROJECT"] = os.getenv(
    "LANGSMITH_PROJECT", "rag_lcel_ollama_optionB"
)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY ì—†ìŒ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Elasticsearch ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ES_URL = "https://localhost:9200"
ES_PASSWORD = "elastic"
ES_INDEX = "rag_chunks"

DENSE_WEIGHT = 0.7
LEX_WEIGHT = 0.3
TOP_N_AFTER_RERANK = 5

# Context ê¸¸ì´ ì œí•œ
MAX_CONTEXT_CHARS = 8000  # â˜… ì›í•˜ëŠ” ê¸¸ì´ë¡œ ì¡°ì ˆ ê°€ëŠ¥ (chars ê¸°ì¤€)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Embedding
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
emb = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenAI Reranker LLM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
reranker_llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY, temperature=0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ollama LLM í˜¸ì¶œ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ollama_chat(prompt: str, model="llama3:8b-instruct-q4_0"):
    """Ollama ëª¨ë¸ í˜¸ì¶œ"""
    r = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False},
        timeout=600,  # â˜… ê¸´ contextì—ì„œë„ timeout ë°©ì§€
    )
    r.raise_for_status()
    return r.json()["response"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompt Loader
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_prompt(name: str, version=None, model=None):
    base_dir = Path(__file__).resolve().parent.parent
    path = base_dir / "prompt" / "prompts.yaml"

    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    candidates = []
    for item in data.get("prompts", []):
        if item.get("name") != name:
            continue
        if version is not None and item.get("version") != version:
            continue
        if model is not None and item.get("model") != model:
            continue
        candidates.append(item)

    if not candidates:
        raise ValueError(f"Prompt not found: {name}, {version}, {model}")

    return candidates[0]["template"]


prompt_template = load_prompt("rag_qa", "1.0.0")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Elasticsearch Connect
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
es = Elasticsearch(ES_URL, basic_auth=("elastic", ES_PASSWORD), verify_certs=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Hybrid Retrieval + OpenAI Reranker
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _normalize(scores):
    mn, mx = min(scores), max(scores)
    if abs(mx - mn) < 1e-9:
        return [1] * len(scores)
    return [(s - mn) / (mx - mn) for s in scores]


def hybrid_retrieve_with_rerank(question: str, k: int = 5):
    log.info(f"[RETRIEVER] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: {question}")

    qvec = emb.embed_query(question)

    # Dense Search
    dense_hits = es.knn_search(
        index=ES_INDEX,
        knn={"field": "embedding", "query_vector": qvec, "k": 10, "num_candidates": 60},
        source=["text", "metadata"],
    )["hits"]["hits"]

    # BM25 Search
    lex_hits = es.search(
        index=ES_INDEX,
        body={
            "size": 10,
            "_source": ["text", "metadata"],
            "query": {
                "multi_match": {
                    "query": question,
                    "fields": ["text^2", "metadata.section"],
                }
            },
        },
    )["hits"]["hits"]

    # Normalize scores
    dense_norm = _normalize([h["_score"] for h in dense_hits]) if dense_hits else []
    lex_norm = _normalize([h["_score"] for h in lex_hits]) if lex_hits else []

    cands = {}

    for hit, s in zip(dense_hits, dense_norm):
        cid = hit["_source"]["metadata"]["chunk_id"]
        cands[cid] = {
            "chunk_id": cid,
            "text": hit["_source"]["text"],
            "meta": hit["_source"]["metadata"],
            "dense": s,
            "lex": 0,
        }

    for hit, s in zip(lex_hits, lex_norm):
        cid = hit["_source"]["metadata"]["chunk_id"]
        if cid not in cands:
            cands[cid] = {
                "chunk_id": cid,
                "text": hit["_source"]["text"],
                "meta": hit["_source"]["metadata"],
                "dense": 0,
                "lex": s,
            }
        else:
            cands[cid]["lex"] = s

    # Hybrid Score
    for c in cands.values():
        c["hybrid"] = DENSE_WEIGHT * c["dense"] + LEX_WEIGHT * c["lex"]

    pool = sorted(cands.values(), key=lambda x: x["hybrid"], reverse=True)[: 2 * k]

    # ğŸ”¥ OpenAI Reranker Prompt
    rerank_prompt = (
        "ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ê²°ê³¼ Rerankerì…ë‹ˆë‹¤.\n"
        "ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë¥¼ 0~5ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
        "ë¬´ì¡°ê±´ JSON ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n\ní›„ë³´ ë¬¸ì„œ:\n"
    )

    for c in pool:
        m = c["meta"]
        rerank_prompt += (
            f"chunk_id={c['chunk_id']} | page={m.get('page')}\n{c['text']}\n\n"
        )

    rerank_scores = {}
    try:
        resp = reranker_llm.invoke(rerank_prompt)
        txt = resp.content.strip().replace("```json", "").replace("```", "")
        for item in json.loads(txt):
            rerank_scores[item["chunk_id"]] = float(item["score"])
    except Exception as e:
        log.warning(f"âš  Reranker ì‹¤íŒ¨ â†’ Hybrid-only ì‚¬ìš© ({e})")

    def rank_key(c):
        return (
            (1, rerank_scores[c["chunk_id"]])
            if c["chunk_id"] in rerank_scores
            else (0, c["hybrid"])
        )

    final = sorted(pool, key=rank_key, reverse=True)[:TOP_N_AFTER_RERANK]

    # Context building
    ctxs = []
    for c in final:
        m = c["meta"]
        ctxs.append(
            f"chunk_id={c['chunk_id']} | page={m.get('page')} | source={m.get('source_file')}\n{c['text']}"
        )

    context_raw = "\n\n---\n\n".join(ctxs)

    # â˜… Context Length Control
    if len(context_raw) > MAX_CONTEXT_CHARS:
        context_raw = context_raw[:MAX_CONTEXT_CHARS] + "\n\n...(ì´í•˜ ìƒëµ)..."

    return context_raw


retriever = RunnableLambda(hybrid_retrieve_with_rerank)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompt Builder
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_prompt(inputs):
    return prompt_template.format(
        context=inputs["context"], question=inputs["question"]
    )


prompt_builder = RunnableLambda(build_prompt)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Postprocess
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_sources(answer: str, ctx: str):
    sources = []
    for block in ctx.split("---"):
        for line in block.split("\n"):
            if line.strip().startswith("chunk_id="):
                sources.append(line.strip())
    sources = list(dict.fromkeys(sources))
    return (
        answer
        + "\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“ ì°¸ê³  ë¬¸ì„œ\n"
        + "\n".join(sources)
    )


postprocess = RunnableLambda(lambda x: extract_sources(x["answer"], x["context"]))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LCEL ì „ì²´ ì²´ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
rag_chain = (
    RunnableParallel(
        question=RunnablePassthrough(),
        context=retriever,
    )
    | RunnableParallel(
        prompt=prompt_builder,
        context=lambda x: x["context"],
    )
    | (lambda x: {"answer": ollama_chat(x["prompt"]), "context": x["context"]})
    | postprocess
)


def answer_with_rag(question: str):
    return rag_chain.invoke(question)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Test
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    q = "2024ë…„ ì—°ë§ì •ì‚° ì¤‘ ìë…€ êµìœ¡ë¹„ ê³µì œ ê¸°ì¤€ê³¼ í•œë„ëŠ”?"
    print(answer_with_rag(q))
