# ===============================================================
# í”„ë¡œê·¸ë¨ëª…: text_split.py
# ê°œìš”: ì „ì²˜ë¦¬ëœ JSON íŒŒì¼ì„ ì½ì–´ë“¤ì—¬ ì²­í‚¹ í›„, JSON íŒŒì¼ë¡œ ì €ì¥
# ì´ë ¥: 2025.12.09 JSON ê¸°ë°˜ ë²„ì „
# ===============================================================
import os
import json
import re
import uuid

# -----------------------------------------------
# âš™ï¸ ì²­í‚¹ ì„¤ì • (ì›í•˜ë©´ ê°’ë§Œ ë°”ê¿”ì„œ ì¬ì¡°ì • ê°€ëŠ¥)
# -----------------------------------------------
MAX_CHARS = 2000
MIN_CHARS = 600
OVERLAP_CHARS = 250
SECTION_BOOST = True  # [SECTION] ë“±ì¥ ì‹œ chunk ê²½ê³„ ê°•í™”

# -----------------------------------------------
# ğŸ“Œ ì…ë ¥ / ì¶œë ¥
# -----------------------------------------------
input_file = r"C:\Users\mungm\Documents\ai_engineer\genai_rag\data\2024ë…„ì›ì²œì§•ìˆ˜ì˜ë¬´ìë¥¼ ìœ„í•œ ì—°ë§ì •ì‚°ì‹ ê³ ì•ˆë‚´_pypdfloader_processing.json"
base_name = os.path.splitext(os.path.basename(input_file))[0]
save_dir = os.path.dirname(input_file)
output_file = os.path.join(save_dir, f"{base_name}_chunks.json")

# -----------------------------------------------
# ğŸ”¹ JSON ì½ê¸°
# -----------------------------------------------
with open(input_file, "r", encoding="utf-8") as f:
    pages = json.load(f)  # [{"page": int, "content": str, "length": int}, ...]

chunks = []
current = ""
current_page = None


def flush_chunk():
    """í˜„ì¬ chunkë¥¼ chunks ë¦¬ìŠ¤íŠ¸ì— ì €ì¥"""
    global current, current_page
    if len(current.strip()) == 0:
        return
    if len(current) < MIN_CHARS and chunks:
        chunks[-1]["chunk"] += "\n" + current
    else:
        chunks.append(
            {
                "chunk_id": str(uuid.uuid4()),
                "chunk": current.strip(),
                "page": current_page,
            }
        )


# -----------------------------------------------
# ğŸ”¹ page ë‹¨ìœ„ content â†’ paragraph ë¶„ë¦¬ í›„ chunking
# -----------------------------------------------
for page_obj in pages:
    page_num = page_obj.get("page")
    text = page_obj.get("content", "")

    # ë‹¨ë½ ë¶„ë¦¬
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

    for para in paragraphs:
        # ì„¹ì…˜ ì‹œì‘ì´ë©´ chunk ê°•ì œ ì¢…ë£Œ
        if SECTION_BOOST and para.startswith("[SECTION]") and len(current) > 0:
            flush_chunk()
            current = para
            current_page = page_num
            continue

        # ì¼ë°˜ ì¶”ê°€
        if len(current) + len(para) + 1 <= MAX_CHARS:
            if not current:
                current_page = page_num
            current += ("\n" if current else "") + para
        else:
            flush_chunk()
            current = para
            current_page = page_num

# ë§ˆì§€ë§‰ ì”ì—¬ chunk ì €ì¥
flush_chunk()

# -----------------------------------------------
# ğŸ”¹ 2ì°¨ ì²­í‚¹: ë„ˆë¬´ ê¸¸ë©´ ì˜¤ë²„ë© ë¶„í• 
# -----------------------------------------------
final_chunks = []
for ch in chunks:
    content = ch["chunk"]
    if len(content) <= MAX_CHARS:
        final_chunks.append(ch)
        continue

    start = 0
    while start < len(content):
        end = start + MAX_CHARS
        piece = content[start:end]
        final_chunks.append(
            {"chunk_id": str(uuid.uuid4()), "chunk": piece.strip(), "page": ch["page"]}
        )
        start = end - OVERLAP_CHARS

# -----------------------------------------------
# ğŸ”¹ SECTION í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë©”íƒ€ë°ì´í„° ìƒì„±
# -----------------------------------------------
for ch in final_chunks:
    m_sec = re.search(r"\[SECTION\]\s*(.+)", ch["chunk"])
    ch["section"] = m_sec.group(1).strip() if m_sec else None
    ch["source_file"] = os.path.basename(input_file)
    ch["length"] = len(ch["chunk"])

# -----------------------------------------------
# ğŸ”¹ JSON ì €ì¥
# -----------------------------------------------
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(final_chunks, f, ensure_ascii=False, indent=2)

print("ì²­í‚¹ ì™„ë£Œ!")
print("ì´ chunk ìˆ˜:", len(final_chunks))
print("ì €ì¥ íŒŒì¼:", output_file)
