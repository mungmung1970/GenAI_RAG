# ===============================================================
# í”„ë¡œê·¸ë¨ëª…: embedding_bge.py  (bge-m3 ë²„ì „)
# ê°œìš”: jsoníŒŒì¼ì„ ì½ì–´ë“¤ì—¬ bge-m3 ì„ë² ë”© ìƒì„± í›„
#       Elasticsearch ì¸ë±ìŠ¤(rag_chunks_bge)ì— ì €ì¥
# ì´ë ¥: 2025.12.08 ìµœì´ˆ ì‘ì„± / bge-m3 ì ìš©
# ===============================================================

import json
import os
import warnings
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from urllib3.exceptions import InsecureRequestWarning
from sentence_transformers import SentenceTransformer

# ğŸ”¹ TLS ê²½ê³  ìˆ¨ê¹€ (ë¡œì»¬ self-signed https ìš©)
warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# â‘  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ES ì ‘ì† ì •ë³´
ES_URL = os.getenv("ES_URL", "https://localhost:9200")
ES_USER = os.getenv("ES_USER", "elastic")
ES_PASSWORD = os.getenv("ES_PASSWORD", "elastic")
ES_INDEX = os.getenv("ES_INDEX", "rag_chunks_bge")  # â˜… bgeìš© ì¸ë±ìŠ¤

# â‘¡ bge-m3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
#    - ê¸°ë³¸ CPU ì‚¬ìš©, GPU ìˆìœ¼ë©´ device="cuda" ë¡œ ë³€ê²½ ê°€ëŠ¥
print("ğŸ”„ bge-m3 ëª¨ë¸ ë¡œë”© ì¤‘ ...")
embed_model = SentenceTransformer(
    "BAAI/bge-m3",
    device="cpu",  # í•„ìš” ì‹œ "cuda"
)
print("âœ… bge-m3 ë¡œë”© ì™„ë£Œ")

# â‘¢ Elasticsearch í´ë¼ì´ì–¸íŠ¸
es = Elasticsearch(
    ES_URL,
    basic_auth=(ES_USER, ES_PASSWORD),
    verify_certs=False,
)

print("ğŸ”— Elasticsearch ì—°ê²° ì™„ë£Œ:", es.info()["version"]["number"])
print(f"ğŸ“Œ ëŒ€ìƒ ì¸ë±ìŠ¤: {ES_INDEX}")

# â‘£ JSON íŒŒì¼ ë¡œë“œ
json_path = r"C:\Users\mungm\Documents\ai_engineer\genai_rag\data\2024ë…„ì›ì²œì§•ìˆ˜ì˜ë¬´ìë¥¼ ìœ„í•œ ì—°ë§ì •ì‚°ì‹ ê³ ì•ˆë‚´_pypdfloader_processing_chunks.json"

with open(json_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

print(f"ğŸ“‚ ë¡œë“œëœ ì²­í¬ ìˆ˜: {len(chunks)}")

# â‘¤ ë°˜ë³µí•˜ë©° embedding ìƒì„± & ES ì €ì¥
for idx, doc in enumerate(chunks, start=1):
    # text / ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    chunk_text = doc["chunk"]
    chunk_id = doc["chunk_id"]
    page = doc.get("page", None)  # JSONì— page í•„ë“œê°€ ìˆë‹¤ê³  ê°€ì •
    section = doc.get("section", None)
    source_file = doc.get("source_file", None)
    length = doc.get("length", len(chunk_text))

    # ğŸ”¹ bge-m3 ì„ë² ë”© ìƒì„± (1024ì°¨ì›)
    #    normalize_embeddings=True â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ì í•©í•˜ê²Œ ì •ê·œí™”
    vec = embed_model.encode(chunk_text, normalize_embeddings=True)
    embedding = vec.tolist()  # numpy ë°°ì—´ â†’ Python list

    # ES ì €ì¥ ë¬¸ì„œ êµ¬ì¡°
    body = {
        "text": chunk_text,
        "metadata": {
            "chunk_id": chunk_id,
            "page": page,
            "source_file": source_file,
            "section": section,
            "length": length,
        },
        "embedding": embedding,
    }

    # id ê¸°ë°˜ upsert
    es.index(index=ES_INDEX, id=chunk_id, document=body)

    if idx % 50 == 0:
        print(f"ğŸ“Œ ì§„í–‰ë¥ : {idx}/{len(chunks)} chunks ì—…ë¡œë“œ ì™„ë£Œ")

# refresh
es.indices.refresh(index=ES_INDEX)
print("ğŸ‰ ëª¨ë“  ì²­í¬ ì„ë² ë”© ë° Elasticsearch ì—…ë¡œë“œ ì™„ë£Œ (bge-m3)")
