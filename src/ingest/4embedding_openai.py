# ===============================================================
# í”„ë¡œê·¸ë¨ëª…: embedding.py
# ê°œìš”: jsoníŒŒì¼ì„ ì½ì–´ë“¤ì—¬ embedding ìƒì„± í›„ Elasticsearch ë²¡í„°DB ì €ì¥
# ì´ë ¥: 2025.12.08 ìµœì´ˆ ì‘ì„± / page ìë™ ì¶”ì¶œ ê¸°ëŠ¥ ì¶”ê°€
# ===============================================================

import json
import os
import re
import warnings
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from openai import OpenAI
from urllib3.exceptions import InsecureRequestWarning

# ğŸ”¹ TLS ê²½ê³  ìˆ¨ê¹€
warnings.filterwarnings("ignore", category=InsecureRequestWarning)

# â‘  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY ì—†ìŒ")

# â‘¡ OpenAI í´ë¼ì´ì–¸íŠ¸ (embedding)
client = OpenAI(api_key=OPENAI_API_KEY)

# â‘¢ Elasticsearch í´ë¼ì´ì–¸íŠ¸
es = Elasticsearch(
    "https://localhost:9200",
    basic_auth=("elastic", "elastic"),
    verify_certs=False,
)

print("ğŸ”— Elasticsearch ì—°ê²° ì™„ë£Œ:", es.info()["version"]["number"])

# â‘£ JSON íŒŒì¼ ë¡œë“œ
json_path = r"C:\Users\mungm\Documents\ai_engineer\genai_rag\data\2024ë…„ì›ì²œì§•ìˆ˜ì˜ë¬´ìë¥¼ ìœ„í•œ ì—°ë§ì •ì‚°ì‹ ê³ ì•ˆë‚´_pypdfloader_processing_chunks.json"

with open(json_path, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# â‘¤ ë°˜ë³µí•˜ë©° embedding ìƒì„± & ES ì €ì¥
for idx, doc in enumerate(chunks, start=1):

    chunk_text = doc["chunk"]

    # ğŸ” [PAGE] 5 ê°™ì€ íŒ¨í„´ ìë™ ì¶”ì¶œ
    page = None
    matches = re.findall(r"\[PAGE\]\s*(\d+)", chunk_text)
    if matches:
        page = int(matches[-1])  # ë§ˆì§€ë§‰ ìˆ«ì ì‚¬ìš© (ê°€ì¥ ìµœì‹  í˜ì´ì§€)
        chunk_text = re.sub(r"\[PAGE\]\s*\d+", "", chunk_text).strip()  # ë³¸ë¬¸ì—ì„œ ì œê±°

    # ì‹¤ì œ OpenAI ì„ë² ë”© í˜¸ì¶œ
    resp = client.embeddings.create(model="text-embedding-3-small", input=chunk_text)
    embedding = resp.data[0].embedding

    # ES ì €ì¥ ë¬¸ì„œ êµ¬ì¡°
    body = {
        "text": chunk_text,
        "metadata": {
            "chunk_id": doc["chunk_id"],
            "page": page,  # ìë™ ì¶”ì¶œëœ page
            "source_file": doc.get("source_file", None),
            "section": doc.get("section", None),
            "length": len(chunk_text),
        },
        "embedding": embedding,
    }

    # id ê¸°ë°˜ upsert
    es.index(index="rag_chunks", id=doc["chunk_id"], document=body)

    if idx % 50 == 0:
        print(f"ğŸ“Œ ì§„í–‰ë¥ : {idx}/{len(chunks)} chunks ì—…ë¡œë“œ ì™„ë£Œ")

# refresh
es.indices.refresh(index="rag_chunks")
print("ğŸ‰ ëª¨ë“  ì²­í¬ ì„ë² ë”© ë° Elasticsearch ì—…ë¡œë“œ ì™„ë£Œ")
